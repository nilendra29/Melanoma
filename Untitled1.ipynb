{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem statement:\n",
    "To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis.\n",
    "\n",
    "Importing all the important libraries\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, InputLayer\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomFlip,RandomRotation, RandomZoom\n",
    "from glob import glob\n",
    "Importing Skin Cancer Data\n",
    "## Mounting Google Drive: #Ref:https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# %pwd\n",
    "# %cd \"/content/gdrive/MyDrive/Google Colab/Upgrad ACPDL/Melanoma Detection Assignment\"\n",
    "# %pwd\n",
    "Mounted at /content/gdrive\n",
    "/content/gdrive/MyDrive/Google Colab/Upgrad ACPDL/Melanoma Detection Assignment\n",
    "'/content/gdrive/MyDrive/Google Colab/Upgrad ACPDL/Melanoma Detection Assignment'\n",
    "This assignment uses a dataset of about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively.\n",
    "\n",
    "# Specify Paths of train and test directory to reuse at multiple places\n",
    "train_path = 'dataset/Train'\n",
    "test_path = 'dataset/Test'\n",
    "# Defining the path for train and test images\n",
    "data_dir_train = pathlib.Path(train_path)\n",
    "data_dir_test = pathlib.Path(test_path)\n",
    "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
    "print(image_count_train)\n",
    "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n",
    "print(image_count_test)\n",
    "2239\n",
    "118\n",
    "Load using keras.preprocessing\n",
    "Let's load these images off disk using the helpful image_dataset_from_directory utility.\n",
    "\n",
    "Create a dataset\n",
    "Define some parameters for the loader:\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "batch_input_shape = (batch_size,img_height,img_width, 3)\n",
    "Use 80% of the images for training, and 20% for validation.\n",
    "\n",
    "validation_split = 0.2\n",
    "## Write your train dataset : load from the specified directory\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(train_path,seed=123,validation_split=validation_split,subset='training',batch_size=batch_size,image_size=(img_height,img_width))\n",
    "Found 2239 files belonging to 9 classes.\n",
    "Using 1792 files for training.\n",
    "## Write your validation dataset: load from the specified directory\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(train_path,seed=123,validation_split=validation_split,subset='validation',batch_size=batch_size,image_size=(img_height,img_width))\n",
    "Found 2239 files belonging to 9 classes.\n",
    "Using 447 files for validation.\n",
    "# List out all the classes of skin cancer and store them in a list. \n",
    "# You can find the class names in the class_names attribute on these datasets. \n",
    "# These correspond to the directory names in alphabetical order.\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "['actinic keratosis', 'basal cell carcinoma', 'dermatofibroma', 'melanoma', 'nevus', 'pigmented benign keratosis', 'seborrheic keratosis', 'squamous cell carcinoma', 'vascular lesion']\n",
    "Visualize the data\n",
    "# Unbatching and picking atmost 1 image per class and storing them for plotting\n",
    "train_ds_unbatched = train_ds.unbatch()\n",
    "image_per_class = {}\n",
    "for elem in train_ds_unbatched.as_numpy_iterator():\n",
    "  if len(image_per_class) >= 9:\n",
    "    break\n",
    "  if elem[1] not in image_per_class:\n",
    "    image_per_class[elem[1]] = elem[0]\n",
    "# Plotting image of each class\n",
    "fig, axes = plt.subplots(3,3,figsize=(15,15))\n",
    "for x in range(3):\n",
    "  for y in range(3):\n",
    "    idx = (x*3) + y\n",
    "    (label,image) = list(image_per_class.items())[idx]\n",
    "    axes[x,y].imshow(image/255.)\n",
    "    axes[x,y].set_title(f\"Image of Disease {class_names[label]}\")\n",
    "\n",
    "DataSet Set up for Better Performance\n",
    "Dataset.cache() keeps the images in memory after they're loaded off disk during the first epoch.\n",
    "\n",
    "Dataset.prefetch() overlaps data preprocessing and model execution while training.\n",
    "\n",
    "The image_batch is a tensor of the shape (32, 180, 180, 3). This is a batch of 32 images of shape 180x180x3 (the last dimension refers to color channels RGB). The label_batch is a tensor of the shape (32,), these are corresponding labels to the 32 images.\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "Create the base model\n",
    "Using layers.experimental.preprocessing.Rescaling to normalize pixel values between (0,1). The RGB channel values are in the [0, 255] range. This is not ideal for a neural network. Here, it is good to standardize values to be in the [0, 1]\n",
    "base_model = Sequential()\n",
    "\n",
    "#Rescaling Layer\n",
    "base_model.add(Rescaling(scale=1./255.))\n",
    "\n",
    "## Feature Learning Layers\n",
    "base_model.add(Conv2D(filters=16, kernel_size = (3, 3), activation='relu', batch_input_shape=batch_input_shape))\n",
    "base_model.add(MaxPool2D(strides=(2,2)))\n",
    "\n",
    "base_model.add(Conv2D(filters=32, kernel_size = (3, 3), activation='relu'))\n",
    "base_model.add(MaxPool2D(strides=(2,2)))\n",
    "\n",
    "base_model.add(Conv2D(filters=64, kernel_size = (3, 3), activation='relu'))\n",
    "base_model.add(MaxPool2D(strides=(2,2)))\n",
    "\n",
    "## Classification Layers\n",
    "base_model.add(Flatten())\n",
    "base_model.add(Dense(128, activation='relu'))\n",
    "base_model.add(Dense(len(class_names),activation='softmax'))\n",
    "\n",
    "# Build the model\n",
    "base_model.build(batch_input_shape)\n",
    "Compile the model with Adam Optimizer and SparseCategoricalCrossentropy For Loss\n",
    "base_model.compile(optimizer=Adam(),\n",
    "              loss=SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "# View the summary of all layers\n",
    "base_model.summary()\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " rescaling (Rescaling)       (32, 180, 180, 3)         0         \n",
    "                                                                 \n",
    " conv2d (Conv2D)             (32, 178, 178, 16)        448       \n",
    "                                                                 \n",
    " max_pooling2d (MaxPooling2D  (32, 89, 89, 16)         0         \n",
    " )                                                               \n",
    "                                                                 \n",
    " conv2d_1 (Conv2D)           (32, 87, 87, 32)          4640      \n",
    "                                                                 \n",
    " max_pooling2d_1 (MaxPooling  (32, 43, 43, 32)         0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " conv2d_2 (Conv2D)           (32, 41, 41, 64)          18496     \n",
    "                                                                 \n",
    " max_pooling2d_2 (MaxPooling  (32, 20, 20, 64)         0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " flatten (Flatten)           (32, 25600)               0         \n",
    "                                                                 \n",
    " dense (Dense)               (32, 128)                 3276928   \n",
    "                                                                 \n",
    " dense_1 (Dense)             (32, 9)                   1161      \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 3,301,673\n",
    "Trainable params: 3,301,673\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train the Base model\n",
    "epochs = 20\n",
    "history = base_model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n",
    "Epoch 1/20\n",
    "56/56 [==============================] - 227s 965ms/step - loss: 2.1843 - accuracy: 0.1780 - val_loss: 2.0142 - val_accuracy: 0.2192\n",
    "Epoch 2/20\n",
    "56/56 [==============================] - 1s 23ms/step - loss: 1.9453 - accuracy: 0.2762 - val_loss: 1.7961 - val_accuracy: 0.3624\n",
    "Epoch 3/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 1.7230 - accuracy: 0.3817 - val_loss: 1.5560 - val_accuracy: 0.4966\n",
    "Epoch 4/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 1.5233 - accuracy: 0.4615 - val_loss: 1.4567 - val_accuracy: 0.5213\n",
    "Epoch 5/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 1.4294 - accuracy: 0.4994 - val_loss: 1.4268 - val_accuracy: 0.5145\n",
    "Epoch 6/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 1.3615 - accuracy: 0.5167 - val_loss: 1.3828 - val_accuracy: 0.5369\n",
    "Epoch 7/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 1.3520 - accuracy: 0.5273 - val_loss: 1.4077 - val_accuracy: 0.5213\n",
    "Epoch 8/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 1.2630 - accuracy: 0.5569 - val_loss: 1.3623 - val_accuracy: 0.5503\n",
    "Epoch 9/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 1.1887 - accuracy: 0.5686 - val_loss: 1.3973 - val_accuracy: 0.5145\n",
    "Epoch 10/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 1.1032 - accuracy: 0.5982 - val_loss: 1.4677 - val_accuracy: 0.5481\n",
    "Epoch 11/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 1.1049 - accuracy: 0.6083 - val_loss: 1.5532 - val_accuracy: 0.4385\n",
    "Epoch 12/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 1.0218 - accuracy: 0.6378 - val_loss: 1.5111 - val_accuracy: 0.5011\n",
    "Epoch 13/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 0.9155 - accuracy: 0.6646 - val_loss: 1.4760 - val_accuracy: 0.5257\n",
    "Epoch 14/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 0.8350 - accuracy: 0.6908 - val_loss: 1.5451 - val_accuracy: 0.5145\n",
    "Epoch 15/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 0.8529 - accuracy: 0.6948 - val_loss: 1.6170 - val_accuracy: 0.5280\n",
    "Epoch 16/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 0.7657 - accuracy: 0.7232 - val_loss: 1.6944 - val_accuracy: 0.5213\n",
    "Epoch 17/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 0.6393 - accuracy: 0.7690 - val_loss: 1.7193 - val_accuracy: 0.5257\n",
    "Epoch 18/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 0.5973 - accuracy: 0.7919 - val_loss: 1.6849 - val_accuracy: 0.5034\n",
    "Epoch 19/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 0.5814 - accuracy: 0.7902 - val_loss: 1.9979 - val_accuracy: 0.5503\n",
    "Epoch 20/20\n",
    "56/56 [==============================] - 1s 22ms/step - loss: 0.6334 - accuracy: 0.7673 - val_loss: 1.8322 - val_accuracy: 0.5235\n",
    "Visualizing training results\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "Findings from Base Model\n",
    "Model clearly overfits. Training Accuracy increases with the number of epochs. But Validation Accuracy becomes more or less stagnant midway.\n",
    "Model learnt noise along with patterns and thus, the training accuracy is high and validation accuracy is low\n",
    "Data Augmentation\n",
    "# Using RandomFlip, RandomRotation and RandomZoom as data augmentation strategies\n",
    "data_augumentation_layers = [\n",
    "    RandomFlip(\"horizontal_and_vertical\"),\n",
    "    RandomRotation(0.3),\n",
    "    RandomZoom(0.5,0.2)\n",
    "]\n",
    "data_augumentation = Sequential(data_augumentation_layers)\n",
    "# Check how Augmentation works (visually) for a sample image from training data\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,12))\n",
    "for image,label in train_ds_unbatched.take(1):\n",
    "  axes[0].imshow(image/255.)\n",
    "  axes[0].set_title(f\"Original Image of class {class_names[label]}\")\n",
    "  img = tf.cast(tf.expand_dims(image, 0), tf.float32)\n",
    "  aug_img = data_augumentation(img)\n",
    "  axes[1].imshow(aug_img[0]/255.)\n",
    "  axes[1].set_title(f\"Augmented Image of class {class_names[label]}\")\n",
    "\n",
    "Create the model, compile and train the Data Augmented (as a layer) model\n",
    "data_aug_model = Sequential()\n",
    "\n",
    "# Data Augumentation layer\n",
    "data_aug_model.add(data_augumentation)\n",
    "\n",
    "#Rescaling Layer\n",
    "data_aug_model.add(Rescaling(scale=1./255.))\n",
    "\n",
    "## Feature Learning Layers\n",
    "## BatchNormalization added to boost up training\n",
    "## Dropouts added to regularize \n",
    "data_aug_model.add(Conv2D(filters=16, kernel_size = (3, 3), activation='relu',batch_input_shape=batch_input_shape))\n",
    "data_aug_model.add(BatchNormalization())\n",
    "data_aug_model.add(MaxPool2D(strides=(2,2)))\n",
    "data_aug_model.add(Dropout(0.25))\n",
    "\n",
    "data_aug_model.add(Conv2D(filters=32, kernel_size = (3, 3), activation='relu'))\n",
    "data_aug_model.add(BatchNormalization())\n",
    "data_aug_model.add(MaxPool2D(strides=(2,2)))\n",
    "data_aug_model.add(Dropout(0.25))\n",
    "\n",
    "data_aug_model.add(Conv2D(filters=64, kernel_size = (3, 3), activation='relu'))\n",
    "data_aug_model.add(BatchNormalization())\n",
    "data_aug_model.add(MaxPool2D(strides=(2,2)))\n",
    "data_aug_model.add(Dropout(0.25))\n",
    "\n",
    "## Classification Layers\n",
    "data_aug_model.add(Flatten())\n",
    "data_aug_model.add(Dense(128, activation='relu'))\n",
    "data_aug_model.add(Dropout(0.4))\n",
    "\n",
    "data_aug_model.add(Dense(len(class_names),activation='softmax'))\n",
    "\n",
    "# Build the model\n",
    "data_aug_model.build(batch_input_shape)\n",
    "Compiling the model\n",
    "data_aug_model.compile(optimizer=Adam(),\n",
    "              loss=SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "# View the Model Summary\n",
    "data_aug_model.summary()\n",
    "Model: \"sequential_2\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " sequential_1 (Sequential)   (None, 180, 180, 3)       0         \n",
    "                                                                 \n",
    " rescaling_1 (Rescaling)     (32, 180, 180, 3)         0         \n",
    "                                                                 \n",
    " conv2d_3 (Conv2D)           (32, 178, 178, 16)        448       \n",
    "                                                                 \n",
    " batch_normalization (BatchN  (32, 178, 178, 16)       64        \n",
    " ormalization)                                                   \n",
    "                                                                 \n",
    " max_pooling2d_3 (MaxPooling  (32, 89, 89, 16)         0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " dropout (Dropout)           (32, 89, 89, 16)          0         \n",
    "                                                                 \n",
    " conv2d_4 (Conv2D)           (32, 87, 87, 32)          4640      \n",
    "                                                                 \n",
    " batch_normalization_1 (Batc  (32, 87, 87, 32)         128       \n",
    " hNormalization)                                                 \n",
    "                                                                 \n",
    " max_pooling2d_4 (MaxPooling  (32, 43, 43, 32)         0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " dropout_1 (Dropout)         (32, 43, 43, 32)          0         \n",
    "                                                                 \n",
    " conv2d_5 (Conv2D)           (32, 41, 41, 64)          18496     \n",
    "                                                                 \n",
    " batch_normalization_2 (Batc  (32, 41, 41, 64)         256       \n",
    " hNormalization)                                                 \n",
    "                                                                 \n",
    " max_pooling2d_5 (MaxPooling  (32, 20, 20, 64)         0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " dropout_2 (Dropout)         (32, 20, 20, 64)          0         \n",
    "                                                                 \n",
    " flatten_1 (Flatten)         (32, 25600)               0         \n",
    "                                                                 \n",
    " dense_2 (Dense)             (32, 128)                 3276928   \n",
    "                                                                 \n",
    " dropout_3 (Dropout)         (32, 128)                 0         \n",
    "                                                                 \n",
    " dense_3 (Dense)             (32, 9)                   1161      \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 3,302,121\n",
    "Trainable params: 3,301,897\n",
    "Non-trainable params: 224\n",
    "_________________________________________________________________\n",
    "Training the model\n",
    "epochs = 20\n",
    "history = data_aug_model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n",
    "Epoch 1/20\n",
    "56/56 [==============================] - 4s 40ms/step - loss: 3.8117 - accuracy: 0.2310 - val_loss: 11.3849 - val_accuracy: 0.2058\n",
    "Epoch 2/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 2.0726 - accuracy: 0.3080 - val_loss: 12.7833 - val_accuracy: 0.2058\n",
    "Epoch 3/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 2.0138 - accuracy: 0.2840 - val_loss: 14.2931 - val_accuracy: 0.2058\n",
    "Epoch 4/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.9182 - accuracy: 0.3047 - val_loss: 6.2398 - val_accuracy: 0.1544\n",
    "Epoch 5/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.9279 - accuracy: 0.3108 - val_loss: 4.9617 - val_accuracy: 0.2192\n",
    "Epoch 6/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.9592 - accuracy: 0.2980 - val_loss: 4.0656 - val_accuracy: 0.1790\n",
    "Epoch 7/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.9084 - accuracy: 0.3119 - val_loss: 2.7011 - val_accuracy: 0.1924\n",
    "Epoch 8/20\n",
    "56/56 [==============================] - 2s 31ms/step - loss: 1.8910 - accuracy: 0.3103 - val_loss: 4.4217 - val_accuracy: 0.1834\n",
    "Epoch 9/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.8832 - accuracy: 0.3276 - val_loss: 5.8740 - val_accuracy: 0.2260\n",
    "Epoch 10/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.9012 - accuracy: 0.3198 - val_loss: 3.8322 - val_accuracy: 0.3356\n",
    "Epoch 11/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.8590 - accuracy: 0.3175 - val_loss: 5.3368 - val_accuracy: 0.3177\n",
    "Epoch 12/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.8491 - accuracy: 0.3214 - val_loss: 2.2772 - val_accuracy: 0.3333\n",
    "Epoch 13/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.8239 - accuracy: 0.3265 - val_loss: 3.5712 - val_accuracy: 0.2729\n",
    "Epoch 14/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.8272 - accuracy: 0.3214 - val_loss: 1.9039 - val_accuracy: 0.3490\n",
    "Epoch 15/20\n",
    "56/56 [==============================] - 2s 31ms/step - loss: 1.7988 - accuracy: 0.3421 - val_loss: 2.5857 - val_accuracy: 0.2752\n",
    "Epoch 16/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.8096 - accuracy: 0.3382 - val_loss: 6.2947 - val_accuracy: 0.1879\n",
    "Epoch 17/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.7447 - accuracy: 0.3449 - val_loss: 5.7250 - val_accuracy: 0.2685\n",
    "Epoch 18/20\n",
    "56/56 [==============================] - 2s 31ms/step - loss: 1.7880 - accuracy: 0.3225 - val_loss: 2.5697 - val_accuracy: 0.3736\n",
    "Epoch 19/20\n",
    "56/56 [==============================] - 2s 32ms/step - loss: 1.8439 - accuracy: 0.3677 - val_loss: 3.2770 - val_accuracy: 0.2349\n",
    "Epoch 20/20\n",
    "56/56 [==============================] - 2s 31ms/step - loss: 1.7724 - accuracy: 0.3521 - val_loss: 2.8159 - val_accuracy: 0.2416\n",
    "Visualizing the results\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "Observation\n",
    "Even though training and validation accuracy came closer, both are of low value. This indicates clearly that model underfits\n",
    "Finding Class Distribution\n",
    "Context: Many times real life datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data.\n",
    "# Fetching the path excluding the output folder\n",
    "path_list = [x for x in glob(os.path.join(data_dir_train, '*', '*.jpg'))]\n",
    "lesion_list = [os.path.basename(os.path.dirname(y)) for y in glob(os.path.join(data_dir_train, '*', '*.jpg'))]\n",
    "# Creating Df with path and label\n",
    "dataframe_dict = dict(zip(path_list, lesion_list))\n",
    "original_df = pd.DataFrame(list(dataframe_dict.items()),columns = ['Path','Label'])\n",
    "original_df['Label'].value_counts()\n",
    "pigmented benign keratosis    462\n",
    "melanoma                      438\n",
    "basal cell carcinoma          376\n",
    "nevus                         357\n",
    "squamous cell carcinoma       181\n",
    "vascular lesion               139\n",
    "actinic keratosis             114\n",
    "dermatofibroma                 95\n",
    "seborrheic keratosis           77\n",
    "Name: Label, dtype: int64\n",
    "# Plot bar chart to understand the number of images in each class\n",
    "original_df['Label'].value_counts().sort_values().plot(kind = 'barh')\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x7fec3e504d10>\n",
    "\n",
    "Findings\n",
    "- Which class has the least number of samples?\n",
    "    seborrheic keratosis\n",
    "- Which classes dominate the data in terms proportionate number of samples?\n",
    "    (In decreasing order)\n",
    "    pigmented benign keratosis\n",
    "    melanoma\n",
    "    basal cell carcinoma \n",
    "    nevus \n",
    "Rectify the class imbalance\n",
    "Context: You can use a python package known as Augmentor (https://augmentor.readthedocs.io/en/master/) to add more samples across all classes so that none of the classes have very few samples.\n",
    "!pip install Augmentor\n",
    "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
    "Collecting Augmentor\n",
    "  Downloading Augmentor-0.2.10-py2.py3-none-any.whl (38 kB)\n",
    "Requirement already satisfied: tqdm>=4.9.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (4.64.0)\n",
    "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (7.1.2)\n",
    "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (0.16.0)\n",
    "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (1.21.6)\n",
    "Installing collected packages: Augmentor\n",
    "Successfully installed Augmentor-0.2.10\n",
    "To use Augmentor, the following general procedure is followed:\n",
    "\n",
    "Instantiate a Pipeline object pointing to a directory containing your initial image data set.\n",
    "Define a number of operations to perform on this data set using your Pipeline object.\n",
    "Execute these operations by calling the Pipeline’s sample() method.\n",
    "path_to_training_dataset=train_path + \"/\"\n",
    "import Augmentor\n",
    "for i in class_names:\n",
    "    p = Augmentor.Pipeline(path_to_training_dataset + i)\n",
    "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
    "    p.sample(500) ## We are adding 500 samples per class to make sure that none of the classes are sparse.\n",
    "Initialised with 114 image(s) found.\n",
    "Output directory set to dataset/Train/actinic keratosis/output.\n",
    "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7FEC3E5E1150>: 100%|██████████| 500/500 [00:20<00:00, 24.11 Samples/s]\n",
    "Initialised with 376 image(s) found.\n",
    "Output directory set to dataset/Train/basal cell carcinoma/output.\n",
    "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7FEC3E60D450>: 100%|██████████| 500/500 [00:22<00:00, 22.30 Samples/s]\n",
    "Initialised with 95 image(s) found.\n",
    "Output directory set to dataset/Train/dermatofibroma/output.\n",
    "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7FEC3E5D5690>: 100%|██████████| 500/500 [00:23<00:00, 21.28 Samples/s]\n",
    "Initialised with 438 image(s) found.\n",
    "Output directory set to dataset/Train/melanoma/output.\n",
    "Processing <PIL.Image.Image image mode=RGB size=1024x768 at 0x7FEBACC9CAD0>: 100%|██████████| 500/500 [01:41<00:00,  4.93 Samples/s]\n",
    "Initialised with 357 image(s) found.\n",
    "Output directory set to dataset/Train/nevus/output.\n",
    "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2012x1956 at 0x7FEBACD13D10>: 100%|██████████| 500/500 [01:30<00:00,  5.52 Samples/s]\n",
    "Initialised with 462 image(s) found.\n",
    "Output directory set to dataset/Train/pigmented benign keratosis/output.\n",
    "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7FEBACC4E610>: 100%|██████████| 500/500 [00:19<00:00, 26.25 Samples/s]\n",
    "Initialised with 77 image(s) found.\n",
    "Output directory set to dataset/Train/seborrheic keratosis/output.\n",
    "Processing <PIL.Image.Image image mode=RGB size=1024x768 at 0x7FEBAA03AC90>: 100%|██████████| 500/500 [00:42<00:00, 11.80 Samples/s]\n",
    "Initialised with 181 image(s) found.\n",
    "Output directory set to dataset/Train/squamous cell carcinoma/output.\n",
    "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x450 at 0x7FEBACC51310>: 100%|██████████| 500/500 [00:18<00:00, 26.80 Samples/s]\n",
    "Initialised with 139 image(s) found.\n",
    "Output directory set to dataset/Train/vascular lesion/output.\n",
    "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x450 at 0x7FEC3E5D52D0>: 100%|██████████| 500/500 [00:18<00:00, 27.24 Samples/s]\n",
    "Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types.. Lets take a look at total count of augmented images.\n",
    "\n",
    "image_count_train = len(list(data_dir_train.glob('*/output/*.jpg')))\n",
    "print(image_count_train)\n",
    "4500\n",
    "Lets see the distribution of augmented data after adding new images to the original training data.\n",
    "path_list_new = [x for x in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]\n",
    "\n",
    "# Look at a sample path \n",
    "print(f\"Length of the path list (= Total number of images added by Augmentor): {len(path_list_new)}\")\n",
    "path_list_new[0]\n",
    "Length of the path list (= Total number of images added by Augmentor): 4500\n",
    "'dataset/Train/melanoma/output/melanoma_original_ISIC_0000303.jpg_31476f67-fb57-471a-932d-d1fa5676e33b.jpg'\n",
    "lesion_list_new = [os.path.basename(os.path.dirname(os.path.dirname(y))) for y in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]\n",
    "# Look at a sample lesion\n",
    "lesion_list_new[0]\n",
    "'melanoma'\n",
    "dataframe_dict_new = dict(zip(path_list_new, lesion_list_new))\n",
    "df2 = pd.DataFrame(list(dataframe_dict_new.items()),columns = ['Path','Label'])\n",
    "new_df = original_df.append(df2)\n",
    "new_df['Label'].value_counts()\n",
    "pigmented benign keratosis    962\n",
    "melanoma                      938\n",
    "basal cell carcinoma          876\n",
    "nevus                         857\n",
    "squamous cell carcinoma       681\n",
    "vascular lesion               639\n",
    "actinic keratosis             614\n",
    "dermatofibroma                595\n",
    "seborrheic keratosis          577\n",
    "Name: Label, dtype: int64\n",
    "new_df['Label'].value_counts().sort_values().plot(kind = 'barh')\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x7feba439c9d0>\n",
    "\n",
    "So, now we have added 500 images to all the classes to maintain some class balance. We can add more images as we want to improve training process.\n",
    "\n",
    "Train the model on the data created using Augmentor\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "Create a training dataset\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir_train, #Already points the train directory\n",
    "  seed=123,\n",
    "  validation_split = 0.2,\n",
    "  subset = 'training',\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "Found 6739 files belonging to 9 classes.\n",
    "Using 5392 files for training.\n",
    "Create a validation dataset\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir_train,\n",
    "  seed=123,\n",
    "  validation_split = 0.2,\n",
    "  subset = 'validation', \n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "Found 6739 files belonging to 9 classes.\n",
    "Using 1347 files for validation.\n",
    "Optimize the data fetch\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "Creating Final Model after dealing with Class Imbalance\n",
    "final_model = Sequential()\n",
    "\n",
    "#Rescaling Layer\n",
    "final_model.add(Rescaling(scale=1./255.))\n",
    "\n",
    "## Feature Learning Layers\n",
    "final_model.add(Conv2D(filters=16, kernel_size = (3, 3), activation='relu',batch_input_shape=batch_input_shape))\n",
    "final_model.add(BatchNormalization())\n",
    "final_model.add(MaxPool2D(strides=(2,2)))\n",
    "final_model.add(Dropout(0.25))\n",
    "\n",
    "final_model.add(Conv2D(filters=32, kernel_size = (3, 3), activation='relu'))\n",
    "final_model.add(BatchNormalization())\n",
    "final_model.add(MaxPool2D(strides=(2,2)))\n",
    "final_model.add(Dropout(0.25))\n",
    "\n",
    "final_model.add(Conv2D(filters=64, kernel_size = (3, 3), activation='relu'))\n",
    "final_model.add(BatchNormalization())\n",
    "final_model.add(MaxPool2D(strides=(2,2)))\n",
    "final_model.add(Dropout(0.25))\n",
    "\n",
    "## Classification Layers\n",
    "final_model.add(Flatten())\n",
    "final_model.add(Dense(128, activation='relu'))\n",
    "final_model.add(Dropout(0.4))\n",
    "\n",
    "final_model.add(Dense(len(class_names),activation='softmax'))\n",
    "\n",
    "# Build the model\n",
    "final_model.build(batch_input_shape)\n",
    "Compile your model (Choose optimizer and loss function appropriately)\n",
    "final_model.compile(optimizer=Adam(),\n",
    "              loss=SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "# Print Model Summary to look at Input Output shapes and parameters\n",
    "final_model.summary()\n",
    "Model: \"sequential_3\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " rescaling_2 (Rescaling)     (32, 180, 180, 3)         0         \n",
    "                                                                 \n",
    " conv2d_6 (Conv2D)           (32, 178, 178, 16)        448       \n",
    "                                                                 \n",
    " batch_normalization_3 (Batc  (32, 178, 178, 16)       64        \n",
    " hNormalization)                                                 \n",
    "                                                                 \n",
    " max_pooling2d_6 (MaxPooling  (32, 89, 89, 16)         0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " dropout_4 (Dropout)         (32, 89, 89, 16)          0         \n",
    "                                                                 \n",
    " conv2d_7 (Conv2D)           (32, 87, 87, 32)          4640      \n",
    "                                                                 \n",
    " batch_normalization_4 (Batc  (32, 87, 87, 32)         128       \n",
    " hNormalization)                                                 \n",
    "                                                                 \n",
    " max_pooling2d_7 (MaxPooling  (32, 43, 43, 32)         0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " dropout_5 (Dropout)         (32, 43, 43, 32)          0         \n",
    "                                                                 \n",
    " conv2d_8 (Conv2D)           (32, 41, 41, 64)          18496     \n",
    "                                                                 \n",
    " batch_normalization_5 (Batc  (32, 41, 41, 64)         256       \n",
    " hNormalization)                                                 \n",
    "                                                                 \n",
    " max_pooling2d_8 (MaxPooling  (32, 20, 20, 64)         0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " dropout_6 (Dropout)         (32, 20, 20, 64)          0         \n",
    "                                                                 \n",
    " flatten_2 (Flatten)         (32, 25600)               0         \n",
    "                                                                 \n",
    " dense_4 (Dense)             (32, 128)                 3276928   \n",
    "                                                                 \n",
    " dropout_7 (Dropout)         (32, 128)                 0         \n",
    "                                                                 \n",
    " dense_5 (Dense)             (32, 9)                   1161      \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 3,302,121\n",
    "Trainable params: 3,301,897\n",
    "Non-trainable params: 224\n",
    "_________________________________________________________________\n",
    "Train the model\n",
    "epochs = 30\n",
    "history = final_model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n",
    "Epoch 1/30\n",
    "169/169 [==============================] - 54s 64ms/step - loss: 2.9591 - accuracy: 0.2235 - val_loss: 8.2648 - val_accuracy: 0.1084\n",
    "Epoch 2/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 1.9264 - accuracy: 0.2878 - val_loss: 6.5309 - val_accuracy: 0.1321\n",
    "Epoch 3/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 1.8243 - accuracy: 0.3190 - val_loss: 2.8951 - val_accuracy: 0.2324\n",
    "Epoch 4/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 1.7379 - accuracy: 0.3364 - val_loss: 5.6895 - val_accuracy: 0.2806\n",
    "Epoch 5/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 1.6657 - accuracy: 0.3531 - val_loss: 3.0406 - val_accuracy: 0.3066\n",
    "Epoch 6/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 1.6454 - accuracy: 0.3635 - val_loss: 2.7699 - val_accuracy: 0.2866\n",
    "Epoch 7/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 1.5758 - accuracy: 0.3817 - val_loss: 4.9876 - val_accuracy: 0.3393\n",
    "Epoch 8/30\n",
    "169/169 [==============================] - 5s 28ms/step - loss: 1.5201 - accuracy: 0.3811 - val_loss: 2.2395 - val_accuracy: 0.2777\n",
    "Epoch 9/30\n",
    "169/169 [==============================] - 5s 28ms/step - loss: 1.5304 - accuracy: 0.3948 - val_loss: 1.5678 - val_accuracy: 0.4313\n",
    "Epoch 10/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 1.4145 - accuracy: 0.4377 - val_loss: 1.8421 - val_accuracy: 0.3898\n",
    "Epoch 11/30\n",
    "169/169 [==============================] - 5s 31ms/step - loss: 1.3500 - accuracy: 0.4518 - val_loss: 1.4827 - val_accuracy: 0.4573\n",
    "Epoch 12/30\n",
    "169/169 [==============================] - 5s 30ms/step - loss: 1.3239 - accuracy: 0.4681 - val_loss: 1.7336 - val_accuracy: 0.4425\n",
    "Epoch 13/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 1.2841 - accuracy: 0.4853 - val_loss: 1.4428 - val_accuracy: 0.5063\n",
    "Epoch 14/30\n",
    "169/169 [==============================] - 5s 28ms/step - loss: 1.2612 - accuracy: 0.5009 - val_loss: 1.2958 - val_accuracy: 0.5382\n",
    "Epoch 15/30\n",
    "169/169 [==============================] - 5s 28ms/step - loss: 1.2119 - accuracy: 0.5167 - val_loss: 1.5960 - val_accuracy: 0.5063\n",
    "Epoch 16/30\n",
    "169/169 [==============================] - 5s 28ms/step - loss: 1.1845 - accuracy: 0.5321 - val_loss: 1.9305 - val_accuracy: 0.3541\n",
    "Epoch 17/30\n",
    "169/169 [==============================] - 5s 28ms/step - loss: 1.2071 - accuracy: 0.5208 - val_loss: 2.7678 - val_accuracy: 0.3653\n",
    "Epoch 18/30\n",
    "169/169 [==============================] - 5s 28ms/step - loss: 1.1222 - accuracy: 0.5406 - val_loss: 1.3954 - val_accuracy: 0.6184\n",
    "Epoch 19/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 1.0445 - accuracy: 0.5705 - val_loss: 1.4154 - val_accuracy: 0.5516\n",
    "Epoch 20/30\n",
    "169/169 [==============================] - 5s 31ms/step - loss: 1.0000 - accuracy: 0.5935 - val_loss: 1.8921 - val_accuracy: 0.5494\n",
    "Epoch 21/30\n",
    "169/169 [==============================] - 5s 28ms/step - loss: 0.9712 - accuracy: 0.6007 - val_loss: 1.4243 - val_accuracy: 0.5627\n",
    "Epoch 22/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 0.9593 - accuracy: 0.6022 - val_loss: 1.5511 - val_accuracy: 0.4714\n",
    "Epoch 23/30\n",
    "169/169 [==============================] - 5s 28ms/step - loss: 0.9232 - accuracy: 0.6200 - val_loss: 1.3150 - val_accuracy: 0.6073\n",
    "Epoch 24/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 0.9027 - accuracy: 0.6337 - val_loss: 3.2464 - val_accuracy: 0.3898\n",
    "Epoch 25/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 0.8849 - accuracy: 0.6354 - val_loss: 1.8490 - val_accuracy: 0.5019\n",
    "Epoch 26/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 0.8640 - accuracy: 0.6406 - val_loss: 2.1876 - val_accuracy: 0.5048\n",
    "Epoch 27/30\n",
    "169/169 [==============================] - 5s 28ms/step - loss: 0.8348 - accuracy: 0.6515 - val_loss: 1.3265 - val_accuracy: 0.6006\n",
    "Epoch 28/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 0.8163 - accuracy: 0.6675 - val_loss: 1.5910 - val_accuracy: 0.5449\n",
    "Epoch 29/30\n",
    "169/169 [==============================] - 5s 28ms/step - loss: 0.8122 - accuracy: 0.6697 - val_loss: 1.6306 - val_accuracy: 0.5850\n",
    "Epoch 30/30\n",
    "169/169 [==============================] - 5s 29ms/step - loss: 0.7840 - accuracy: 0.6728 - val_loss: 0.9941 - val_accuracy: 0.6748\n",
    "Visualize the model results\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "Observations:\n",
    "Yes Addressing Class Imbalance tries to improve both Training and Validation Accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
